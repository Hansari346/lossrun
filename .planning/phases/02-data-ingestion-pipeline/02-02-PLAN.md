---
phase: 02-data-ingestion-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/lib/sheet-analysis.ts
  - src/lib/composite-fields.ts
  - src/lib/field-mapping.ts
autonomous: true

must_haves:
  truths:
    - "Multi-sheet workbooks auto-rank sheets by claims-data likelihood — the sheet with most rows, claims-related headers, and a claims keyword in its name scores highest"
    - "Columns containing consistent 'Key: Value' patterns (like 'Nature of Injury: Strain') are detected as composite fields with extracted keys listed"
    - "Column auto-mapping correctly matches headers from AMIC, Standard Loss Run, and Sedgwick formats using the expanded synonym dictionary"
  artifacts:
    - path: "src/lib/sheet-analysis.ts"
      provides: "rankSheets() function returning SheetScore[]"
      exports: ["rankSheets"]
      min_lines: 50
    - path: "src/lib/composite-fields.ts"
      provides: "detectCompositeFields() function returning CompositeField[]"
      exports: ["detectCompositeFields"]
      min_lines: 40
    - path: "src/lib/field-mapping.ts"
      provides: "Expanded synonym dictionary with 15+ hints per field"
      contains: "insured location"
  key_links:
    - from: "src/lib/sheet-analysis.ts"
      to: "src/types/index.ts"
      via: "import SheetScore"
      pattern: "import.*SheetScore.*from.*types"
    - from: "src/lib/composite-fields.ts"
      to: "src/types/index.ts"
      via: "import CompositeField"
      pattern: "import.*CompositeField.*from.*types"
---

<objective>
Build the smart sheet selection, composite field detection, and expanded column matching capabilities that let the tool handle multi-sheet workbooks and carrier-specific column naming conventions.

Purpose: INGEST-03 (multi-sheet detection), INGEST-02 (composite field parsing), and INGEST-01 (expanded synonym dictionary) are the intelligence layer that transforms a dumb "pick first sheet, hope headers match" approach into one that reliably finds the right data in any carrier format.

Output: Three modules — sheet-analysis for ranking sheets, composite-fields for Key:Value extraction, and an enhanced field-mapping with comprehensive synonym coverage.
</objective>

<execution_context>
@~/.cursor/get-shit-done/workflows/execute-plan.md
@~/.cursor/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-ingestion-pipeline/02-RESEARCH.md
@src/lib/field-mapping.ts
@src/types/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create smart sheet analysis module</name>
  <files>src/lib/sheet-analysis.ts</files>
  <action>
  Create `src/lib/sheet-analysis.ts` with one primary exported function:

  **`rankSheets(wb: any): SheetScore[]`** — Score and rank all sheets in a workbook by claims-data likelihood. Returns sorted array (highest score first).

  For each sheet in `wb.SheetNames`:

  1. **Skip non-worksheets:** If `ws['!type']` exists and is not `'sheet'`, score -100 with reason. If `ws['!ref']` is missing (empty sheet), score -100.

  2. **Dimension scoring** using `XLSX.utils.decode_range(ws['!ref'])`:
     - `rowCount > 10` → +20 points, reason: `"${rowCount} rows"`
     - `rowCount > 50` → +15 more
     - `rowCount > 500` → +10 more
     - `colCount >= 5 && colCount <= 50` → +15 points, reason: `"${colCount} columns"`
     - `colCount < 3` → -20 points, reason: `"Too few columns"`

  3. **Sheet name scoring** (case-insensitive):
     - Claims keywords: `['claim', 'loss', 'detail', 'data', 'run', 'listing', 'report']` → +25 if any match
     - Summary keywords: `['summary', 'cover', 'total', 'index', 'toc', 'instruction', 'contents', 'pivot', 'chart', 'graph', 'about', 'notes']` → -20 if any match

  4. **Header pattern matching:** Read first 5 rows via `XLSX.utils.sheet_to_json(ws, { header: 1, range: 0, raw: false })`. Scan cells for claims-related keywords: `['claim', 'date', 'loss', 'incurred', 'paid', 'reserve', 'location', 'site', 'body part', 'injury', 'description', 'accident']`. Count hits. If `hits >= 3` → +30 points, reason: `"${hits} header keyword matches"`.

  5. Sort by score descending, return.

  Import `SheetScore` from `../types`. Use the global `XLSX` (declared in types/globals.d.ts). Pure function — no signal store access.

  **Important:** When scanning `sheet_to_json` output, guard against non-array rows and limit scan to first 5 rows to keep it fast on large workbooks.
  </action>
  <verify>
  Run `npx tsc --noEmit` — no type errors. Verify the function signature matches: `rankSheets(wb: any): SheetScore[]`.
  </verify>
  <done>
  src/lib/sheet-analysis.ts exports `rankSheets`. Given a multi-sheet workbook, it returns sheets sorted by claims-data likelihood. A sheet named "Claims Detail" with 200 rows and 15 columns containing "Date of Loss" and "Total Incurred" headers scores highest. Summary/cover sheets score lowest.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create composite field detection module</name>
  <files>src/lib/composite-fields.ts</files>
  <action>
  Create `src/lib/composite-fields.ts` with one primary exported function:

  **`detectCompositeFields(columnIndices: number[], headers: string[], sampleData: any[][], minKeyFrequency?: number): CompositeField[]`**

  For each column index in `columnIndices`:
  1. Get sample values from `sampleData[colIdx]`
  2. For each non-empty sample value, attempt to match the pattern `/^([A-Za-z][A-Za-z\s]{2,30}):\s*(.+)$/`
     - This matches "Key: Value" where the key starts with a letter, is 3-31 chars, followed by `: ` and a value
  3. Accumulate key frequencies in a Map<string, number>
  4. **Filter out false positives:**
     - Exclude keys that are just numbers (`/^\d+$/`)
     - Exclude keys shorter than 3 characters
     - Exclude timestamp-like patterns (the regex already excludes keys starting with numbers)
  5. A column is composite if at least one key appears in `minKeyFrequency` or more rows (default: 3)
  6. Return the array of CompositeField results with `extractedKeys` (keys that met the frequency threshold) sorted alphabetically

  Also export a helper:

  **`extractCompositeValue(cellValue: string, targetKey: string): string | null`** — Given a cell value like "Nature of Injury: Strain" and targetKey "Nature of Injury", returns "Strain". Returns null if the cell doesn't match the target key pattern. This will be used during row parsing to extract values from composite columns.

  Import `CompositeField` from `../types`. Pure functions, no state access.
  </action>
  <verify>
  Run `npx tsc --noEmit` — no type errors. Verify:
  - `detectCompositeFields([0], ["Generic Field 1"], [["Nature of Injury: Strain", "Nature of Injury: Laceration", "Nature of Injury: Contusion", "Nature of Injury: Sprain"]])` → returns 1 CompositeField with extractedKeys containing "Nature of Injury"
  - `extractCompositeValue("Nature of Injury: Strain", "Nature of Injury")` → returns "Strain"
  - `extractCompositeValue("Some random text", "Nature of Injury")` → returns null
  </verify>
  <done>
  src/lib/composite-fields.ts exports `detectCompositeFields` and `extractCompositeValue`. Composite fields are detected by consistent Key:Value patterns across sample rows. False positives (timestamps, short keys, one-off keys) are filtered out. extractCompositeValue cleanly splits a matching cell value.
  </done>
</task>

<task type="auto">
  <name>Task 3: Expand synonym dictionary in field-mapping</name>
  <files>src/lib/field-mapping.ts</files>
  <action>
  Enhance the `hints` arrays in both `requiredFields` and `optionalFields` in `src/lib/field-mapping.ts` to cover real carrier naming conventions from AMIC, Standard Loss Run, and Sedgwick formats.

  **Replace the existing hints with these expanded versions:**

  `requiredFields.site_name.hints`:
  ```
  ['site', 'location', 'facility', 'store', 'city', 'place', 'address', 'branch', 'plant', 'warehouse', 'division', 'department', 'dept', 'insured location', 'risk location', 'loc', 'work location', 'reporting location', 'office', 'region', 'unit']
  ```

  `requiredFields.date_of_loss.hints`:
  ```
  ['date of loss', 'loss date', 'date of injury', 'doi', 'date loss', 'incident date', 'accident date', 'occurrence date', 'claim date', 'injury date', 'date of accident', 'dol', 'date of occurrence', 'event date', 'date of incident', 'occ date', 'accident dt', 'loss dt', 'date occurred']
  ```

  `requiredFields.total_incurred.hints`:
  ```
  ['total incurred', 'net incurred', 'all gross incurred', 'incurred', 'total incur', 'incurred total', 'total loss', 'loss amount', 'claim amount', 'total cost', 'total paid incurred', 'gross incurred', 'incurred amount', 'total inc', 'ttl incurred', 'total claim', 'cumulative incurred', 'inc total']
  ```

  `optionalFields.claim_number.hints`:
  ```
  ['claim number', 'claim #', 'cnr', 'claim num', 'claim id', 'claim no', 'claim#', 'case number', 'case #', 'file number', 'clm number', 'clm #', 'clm no', 'reference number', 'ref #', 'claim ref', 'claim reference', 'file #', 'file no']
  ```

  `optionalFields.claim_category.hints`:
  ```
  ['claim type', 'derived claim type', 'coverage line', 'category', 'classification', 'claim category', 'mo indemnity', 'medical only', 'claim status type', 'loss type', 'coverage', 'line of business', 'lob', 'claim class', 'mo vs indemnity', 'med only', 'indemnity', 'type of claim']
  ```

  `optionalFields.body_part.hints`:
  ```
  ['body part', 'part of body', 'bodypart', 'part body', 'injury part', 'affected part', 'anatomy', 'part of body injured', 'body part injured', 'body part code', 'injured body part', 'injured part', 'body area']
  ```

  `optionalFields.cause_of_loss.hints`:
  ```
  ['loss category', 'cause bucket', 'loss bucket', 'cause category', 'loss type', 'accident category', 'incident category', 'injury category', 'loss classification', 'cause classification', 'loss code', 'cause code', 'cause of injury', 'cause of loss', 'nature of injury', 'injury type', 'accident type', 'injury cause', 'coi', 'noi', 'hazard']
  ```

  `optionalFields.lost_days.hints`:
  ```
  ['lost days', 'days lost', 'disability days', 'lost time days', 'lt days', 'days disability', 'work days lost', 'days off', 'lost work', 'time loss days', 'ttd days', 'lost workdays', 'temporary total disability days', 'days away']
  ```

  `optionalFields.loss_description.hints`:
  ```
  ['description', 'loss description', 'accident description', 'incident description', 'notes', 'comments', 'narrative', 'details', 'injury description', 'cause description', 'description of injury', 'how injury occurred', 'desc', 'loss desc', 'accident desc', 'claim description']
  ```

  **Do NOT change** the scoring logic in `calculateMatchScore`, `detectColumnType`, or `findBestMatch` — only expand the hint arrays. The existing fuzzy matching algorithm is already tuned for this domain.
  </action>
  <verify>
  Run `npx tsc --noEmit` — no type errors. Count hints: `site_name` should have 21 hints, `date_of_loss` should have 19, `total_incurred` should have 18. Verify by reading the file and counting array entries.
  </verify>
  <done>
  All 9 field definitions (3 required + 6 optional) have expanded synonym dictionaries covering AMIC, Standard Loss Run, Sedgwick, and WCIO standard column naming conventions. No logic changes — only data expansion. Auto-mapping accuracy improves for diverse carrier formats.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. `src/lib/sheet-analysis.ts` exists and exports `rankSheets`
3. `src/lib/composite-fields.ts` exists and exports `detectCompositeFields` and `extractCompositeValue`
4. `src/lib/field-mapping.ts` has expanded hints (spot-check: `site_name.hints` includes `'insured location'`, `date_of_loss.hints` includes `'dol'`, `total_incurred.hints` includes `'gross incurred'`)
5. All three modules are pure functions with no signal store imports
6. `npx wrangler dev` starts without errors
</verification>

<success_criteria>
- rankSheets scores a sheet named "Claims Detail" with 200 rows and claims-related headers higher than a sheet named "Summary" with 5 rows
- detectCompositeFields identifies "Nature of Injury: Strain" patterns in Generic Field columns
- extractCompositeValue correctly splits "Nature of Injury: Strain" → "Strain"
- Expanded synonyms cover at least 15 hints per required field and 10+ per optional field
- Zero new dependencies
- Existing auto-mapping behavior preserved (brownfield guard)
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-ingestion-pipeline/02-02-SUMMARY.md`
</output>
